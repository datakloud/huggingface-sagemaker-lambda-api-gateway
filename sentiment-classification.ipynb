{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b083d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run this in terminal\n",
    "\n",
    "# source activate pytorch_p38\n",
    "# /home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip\n",
    "# pip install \"transformers==4.4.2\" \"datasets[s3]==1.5.0\"\n",
    "# pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab896e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sagemaker\n",
    "import string\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf644f32",
   "metadata": {},
   "source": [
    "## 1. SageMaker set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# The SageMaker session bucket is used for uploading data, models and logs\n",
    "sagemaker_session_bucket = <<S3 bucket name>>\n",
    "# SageMaker will automatically create this bucket if it doesn't exist\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # Set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# Get sagemaker execution role\n",
    "role = sagemaker.get_execution_role()\n",
    "# add the default bucket to the session\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"Bucket: {sess.default_bucket()}\")\n",
    "print(f\"Region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8259f9",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d7174",
   "metadata": {},
   "source": [
    "### 2.1 Enrich data with offensive tweets\n",
    "Enriching the sentiment data with offensive language tweets from twitter. The data is available at Hate Speech and Offensive Language Repository [here](https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the offensive data sample from the data folder\n",
    "data = pd.read_csv('./data/offensive_data.csv', usecols=['class', 'tweet'])\n",
    "# Get offensive tweets only\n",
    "offensive_text = data[data['class'] == 1]\n",
    "offensive_text['class'] = 0\n",
    "\n",
    "# Clean the tweet data by removing handles name, emojis etc\n",
    "offensive_text['tweet'] = offensive_text['tweet'].str.replace('!', '')\n",
    "offensive_text['tweet'] = offensive_text['tweet'].str.replace('\"', '')\n",
    "offensive_text['tweet'] = offensive_text['tweet'].str.replace(\"'\", \"\")\n",
    "offensive_text['tweet'] = offensive_text['tweet'].str.replace('  ', ' ')\n",
    "offensive_text['tweet'] = offensive_text['tweet'].str.replace('RT', '')\n",
    "offensive_text['tweet'] = [re.sub('&#[^\\s]+', '', x) for x in offensive_text['tweet']]\n",
    "offensive_text['tweet'] = [re.sub('@[^\\s]+', '', x) for x in offensive_text['tweet']]\n",
    "offensive_text['tweet'] = [re.sub('#[^\\s]+', '', x) for x in offensive_text['tweet']]\n",
    "offensive_text['tweet'] = [re.sub('http\\S+', '', x) for x in offensive_text['tweet']]\n",
    "\n",
    "# Clean the rest and keep only English words\n",
    "offensive_text['tweet'] = [clean_text(x) for x in offensive_text['tweet']]\n",
    "offensive_text.columns = ['label', 'text']\n",
    "\n",
    "# Split the data to train and test, then add to the SST2 dataset. \n",
    "msk = np.random.rand(len(offensive_text)) < 0.8\n",
    "enrich_train = offensive_text[msk]\n",
    "enrich_test = offensive_text[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7edcc1",
   "metadata": {},
   "source": [
    "### 2.2 Download The Stanford Sentiment Treebank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632230a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the SST2 data from s3\n",
    "!curl https://sagemaker-sample-files.s3.amazonaws.com/datasets/text/SST2/sst2.test > ./data/sst2.test\n",
    "!curl https://sagemaker-sample-files.s3.amazonaws.com/datasets/text/SST2/sst2.train > ./data/sst2.train\n",
    "!curl https://sagemaker-sample-files.s3.amazonaws.com/datasets/text/SST2/sst2.val > ./data/sst2.val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e984b3",
   "metadata": {},
   "source": [
    "## Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer used in pre-processing\n",
    "tokenizer_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# S3 key prefix for the data\n",
    "s3_prefix = \"dataset/sst\"\n",
    "\n",
    "# Download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# Tokenizer helper function to tokenize sentences to max 54 words (median text length is 54 words.\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], max_length=54, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "test_df = pd.read_csv(\"./data/sst2.test\", sep=\"delimiter\", header=None, engine=\"python\", names=[\"line\"])\n",
    "train_df = pd.read_csv(\"./data/sst2.train\", sep=\"delimiter\", header=None, engine=\"python\", names=[\"line\"])\n",
    "\n",
    "test_df[[\"label\", \"text\"]] = test_df[\"line\"].str.split(\" \", 1, expand=True)\n",
    "train_df[[\"label\", \"text\"]] = train_df[\"line\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "test_df.drop(\"line\", axis=1, inplace=True)\n",
    "train_df.drop(\"line\", axis=1, inplace=True)\n",
    "\n",
    "test_df[\"label\"] = pd.to_numeric(test_df[\"label\"], downcast=\"integer\")\n",
    "train_df[\"label\"] = pd.to_numeric(train_df[\"label\"], downcast=\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add offensive tweets to the SST2 dataset\n",
    "train_df = pd.concat([train_df, enrich_train], axis=0)\n",
    "test_df = pd.concat([test_df, enrich_test], axis=0)\n",
    "\n",
    "# Reshuffle the datasets\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Convert pandas dataframe to dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d230b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Set format for pytorch\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train/\"\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/test/\"\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85732ae8",
   "metadata": {},
   "source": [
    "## 3. Fine-tune the model and start a SageMaker training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Hyperparameters which are passed into the training job\n",
    "hyperparameters = {\"epochs\": 3, \n",
    "                   \"train_batch_size\": 8,\n",
    "                   \"seed\": 0,\n",
    "                   \"model_name\": \"distilbert-base-uncased\"}\n",
    "\n",
    "base_job_name = \"huggingface-sentiment-project\" # training job name\n",
    "output_path = f\"s3://{sess.default_bucket()}/output/\" # output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604062f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = HuggingFace(\n",
    "    entry_point = \"train.py\", # the training script name\n",
    "    source_dir = \"./scripts\", # the training script location\n",
    "    base_job_name = base_job_name, # training job name\n",
    "    output_path = output_path, # output directory\n",
    "    instance_type = \"ml.p3.2xlarge\", # training instance\n",
    "    instance_count = 1, # number of training instance\n",
    "    volume_size = 100, # disk size of the training instance to hold data and model files temporarily\n",
    "    role = role, # sagemaker role\n",
    "    pytorch_version = \"1.9\", # version of pytorch library\n",
    "    py_version = \"py38\", # version of python\n",
    "    transformers_version = \"4.12\", # version of the transformers library \n",
    "    hyperparameters = hyperparameters, # hyperparameters defined in previous step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5385b9-11a9-4be7-bf9f-3323502f2682",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the training job with the uploaded dataset as input\n",
    "estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(1, \"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"inputs\": \"This is bad experience and customer service was rude\"}\n",
    "\n",
    "predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "content_type = \"application/json\"\n",
    "\n",
    "data = json.loads(json.dumps(payload))\n",
    "payload = json.dumps(data)\n",
    "\n",
    "endpoint_name = <<model endpoint name>>\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=payload)\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "\n",
    "if result[0]['label'] == 'LABEL_1':\n",
    "    output = {\"outcome\": \"Positive\"}\n",
    "else:\n",
    "    output = {\"outcome\": \"Negative\"}\n",
    "\n",
    "print(payload)\n",
    "print(result)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbcd2ba-cd8b-491d-8815-9eb346279747",
   "metadata": {},
   "source": [
    "### Deploy the model using `model_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535e0e0-1710-4536-9441-045157cc9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = 's3://<<S3 bucket name>>/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd7bc2-c92b-4735-9e76-5ac2132c72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data = model_data,  # path to your trained sagemaker model\n",
    "   role = role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version = \"4.12\", # transformers version used\n",
    "   pytorch_version = \"1.9\", # pytorch version used\n",
    "   py_version = \"py38\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941a33a-6b67-4525-b827-3292d7449b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.t2.medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb3112-aae0-4cfb-a543-c019c585756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"New Avatar movie is too long and I felt so bored watching it\"\n",
    "}\n",
    "\n",
    "# request\n",
    "result = predictor.predict(data)\n",
    "\n",
    "if result[0]['label'] == 'LABEL_1':\n",
    "    output = {\"outcome\": \"Positive\"}\n",
    "else:\n",
    "    output = {\"outcome\": \"Negative\"}\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794be8c-f038-4db4-9cbb-a155597cccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
